import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping,ModelCheckpoint
from sklearn.metrics import accuracy_score


tf.keras.backend.clear_session()

# Daten einlesen
data = pd.read_csv('model_new.csv')

data.head()

data['activity'].value_counts().plot(kind='bar', title='Activity type')

def plot_activity(activity, df):
    data = df[df['activity'] == activity][['R1_1', 'R1_2', 'R1_3', 'R1_4', 'R1_5', 'R1_6', 'R3_1', 'R3_2', 'R3_3', 'R3_4', 'R3_5', 'R3_6', 'R4_1', 'R4_2', 'R4_3', 'R4_4', 'R4_5', 'R4_6']]
    axis = data.plot(subplots=True, figsize=(16, 12), title=activity)
    for ax in axis:
        ax.legend(loc='lower left', bbox_to_anchor=(1.0, 0.5))

plot_activity("drilling", data)


# Preprocessing
X = data.drop(['class', 'activity'], axis=1)
y = data['class']
activity_labels = data['activity']
num_classes = 3


def create_sliding_windows(data, labels, activity_labels, window_size, step_size):
    if len(data) < window_size or len(labels) < window_size or len(activity_labels) < window_size:
        raise ValueError("Input data are shorter than window")

    start = 0
    end = window_size
    X_windows = []
    y_windows = []
    activity_labels_windows = []
    while end <= len(data):
        X_windows.append(data[start:end])
        y_windows.append(labels[end-1])
        activity_labels_windows.append(activity_labels[start:end])
        start += step_size
        end += step_size

    return X_windows, y_windows, activity_labels_windows

window_size =55
step_size =10
X_windows, y_windows, activity_labels_windows = create_sliding_windows(X, y, activity_labels, window_size, step_size)


print(len(X_windows))
print(len(y_windows))
print(len(activity_labels_windows))

# Print the contents of each sliding window
for i, window in enumerate(X_windows):
    print(f"Window {i+1}:\n{window}")
    print("=" * 80)

# Split data
X_train, X_test, y_train, y_test, activity_labels_train, activity_labels_test = train_test_split(
    X_windows, y_windows, activity_labels_windows,
    test_size=0.2, stratify=y_windows
)
print(len(activity_labels_test))


print(len(X_train))
print(len(X_test))

X_train = np.array(X_train)
X_test = np.array(X_test)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train.reshape(-1, X.shape[1])).reshape(X_train.shape)
X_test = scaler.transform(X_test.reshape(-1, X.shape[1])).reshape(X_test.shape)

# Build LSTM model
import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.LSTM(32, input_shape=(window_size, X_train.shape[2]), return_sequences=True, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.LSTM(16, activation='relu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
print(model.summary())

# Define Callbacks
callbacks = [
    
    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)
]

# Train the model
history = model.fit(
    X_train, y_train,
    batch_size=64,
    epochs=100,
    validation_split=0.1,
    callbacks=callbacks,
    verbose=1
)


